{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-08T22:45:36.806966Z","iopub.execute_input":"2022-07-08T22:45:36.807412Z","iopub.status.idle":"2022-07-08T22:45:36.813469Z","shell.execute_reply.started":"2022-07-08T22:45:36.807365Z","shell.execute_reply":"2022-07-08T22:45:36.812287Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Imports\nimport cv2\nimport os\nfrom tqdm import tqdm\nfrom glob import glob\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D","metadata":{"execution":{"iopub.status.busy":"2022-07-08T22:45:55.184264Z","iopub.execute_input":"2022-07-08T22:45:55.185217Z","iopub.status.idle":"2022-07-08T22:46:00.663662Z","shell.execute_reply.started":"2022-07-08T22:45:55.185157Z","shell.execute_reply":"2022-07-08T22:46:00.662597Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/state-farm-distracted-driver-detection/driver_imgs_list.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T22:46:04.811030Z","iopub.execute_input":"2022-07-08T22:46:04.811845Z","iopub.status.idle":"2022-07-08T22:46:04.872587Z","shell.execute_reply.started":"2022-07-08T22:46:04.811798Z","shell.execute_reply":"2022-07-08T22:46:04.871670Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"  subject classname            img\n0    p002        c0  img_44733.jpg\n1    p002        c0  img_72999.jpg\n2    p002        c0  img_25094.jpg\n3    p002        c0  img_69092.jpg\n4    p002        c0  img_92629.jpg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>classname</th>\n      <th>img</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>p002</td>\n      <td>c0</td>\n      <td>img_44733.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>p002</td>\n      <td>c0</td>\n      <td>img_72999.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>p002</td>\n      <td>c0</td>\n      <td>img_25094.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>p002</td>\n      <td>c0</td>\n      <td>img_69092.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>p002</td>\n      <td>c0</td>\n      <td>img_92629.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Group bt Drivers/ Test Subjects\nby_drivers = df.groupby('subject')\n\nunique_drivers = by_drivers.groups.keys()\n\nprint(\"There are: \", len(unique_drivers), \" unique drivers\")\nprint('There is a mean of ',round(by_drivers.count()['classname']), ' images by driver.')","metadata":{"execution":{"iopub.status.busy":"2022-07-08T22:46:07.477078Z","iopub.execute_input":"2022-07-08T22:46:07.477564Z","iopub.status.idle":"2022-07-08T22:46:07.510059Z","shell.execute_reply.started":"2022-07-08T22:46:07.477521Z","shell.execute_reply":"2022-07-08T22:46:07.509076Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"There are:  26  unique drivers\nThere is a mean of  subject\np002     725\np012     823\np014     876\np015     875\np016    1078\np021    1237\np022    1233\np024    1226\np026    1196\np035     848\np039     651\np041     605\np042     591\np045     724\np047     835\np049    1011\np050     790\np051     920\np052     740\np056     794\np061     809\np064     820\np066    1034\np072     346\np075     814\np081     823\nName: classname, dtype: int64  images by driver.\n","output_type":"stream"}]},{"cell_type":"code","source":"NUMBER_CLASSES = 10","metadata":{"execution":{"iopub.status.busy":"2022-07-08T22:46:09.775005Z","iopub.execute_input":"2022-07-08T22:46:09.775594Z","iopub.status.idle":"2022-07-08T22:46:09.784275Z","shell.execute_reply.started":"2022-07-08T22:46:09.775550Z","shell.execute_reply":"2022-07-08T22:46:09.783040Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Read with opencv\ndef get_cv2_image(path, img_rows, img_cols, color_type=3):\n    \"\"\"\n    Function that return an opencv image from the path and the right number of dimension\n    \"\"\"\n    if color_type == 1: # Loading as Grayscale image\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    elif color_type == 3: # Loading as color image\n        img = cv2.imread(path, cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Converts to RGB\n\n    img = cv2.resize(img, (img_rows, img_cols)) # Reduce size\n    return img\n\n# Loading Training dataset\ndef load_train(img_rows, img_cols, color_type=3):\n    \"\"\"\n    Return train images and train labels from the original path\n    \"\"\"\n    train_images = [] \n    train_labels = []\n    # Loop over the training folder \n    for classed in tqdm(range(NUMBER_CLASSES)):\n        print('Loading directory c{}'.format(classed))\n        files = glob(os.path.join('../input/state-farm-distracted-driver-detection/imgs/train/c' + str(classed), '*.jpg'))\n        for file in files:\n            img = get_cv2_image(file, img_rows, img_cols, color_type)\n            train_images.append(img)\n            train_labels.append(classed)\n    return train_images, train_labels \n\ndef read_and_normalize_train_data(img_rows, img_cols, color_type):\n    \"\"\"\n    Load + categorical + split\n    \"\"\"\n    X, labels = load_train(img_rows, img_cols, color_type)\n    y = np_utils.to_categorical(labels, 10) #categorical train label\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split into train and test\n    x_train = np.array(x_train, dtype=np.uint8).reshape(-1,img_rows,img_cols,color_type)\n    x_test = np.array(x_test, dtype=np.uint8).reshape(-1,img_rows,img_cols,color_type)\n    \n    return x_train, x_test, y_train, y_test\n\n# Loading validation dataset\ndef load_test(size=200000, img_rows=64, img_cols=64, color_type=3):\n    \"\"\"\n    Same as above but for validation dataset\n    \"\"\"\n    path = os.path.join('../input/state-farm-distracted-driver-detection/imgs/test', '*.jpg')\n    files = sorted(glob(path))\n    X_test, X_test_id = [], []\n    total = 0\n    files_size = len(files)\n    for file in tqdm(files):\n        if total >= size or total >= files_size:\n            break\n        file_base = os.path.basename(file)\n        img = get_cv2_image(file, img_rows, img_cols, color_type)\n        X_test.append(img)\n        X_test_id.append(file_base)\n        total += 1\n    return X_test, X_test_id\n\ndef read_and_normalize_sampled_test_data(size, img_rows, img_cols, color_type=3):\n    test_data, test_ids = load_test(size, img_rows, img_cols, color_type)   \n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.reshape(-1,img_rows,img_cols,color_type)\n    return test_data, test_ids","metadata":{"execution":{"iopub.status.busy":"2022-07-08T22:46:11.305138Z","iopub.execute_input":"2022-07-08T22:46:11.305627Z","iopub.status.idle":"2022-07-08T22:46:11.325630Z","shell.execute_reply.started":"2022-07-08T22:46:11.305590Z","shell.execute_reply":"2022-07-08T22:46:11.324368Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# dimension of images\nimg_rows = 128 \nimg_cols = 128\n\ncolor_type = 1 # grey\nnb_test_samples = 200","metadata":{"execution":{"iopub.status.busy":"2022-07-08T22:46:12.778352Z","iopub.execute_input":"2022-07-08T22:46:12.779078Z","iopub.status.idle":"2022-07-08T22:46:12.785123Z","shell.execute_reply.started":"2022-07-08T22:46:12.779040Z","shell.execute_reply":"2022-07-08T22:46:12.784072Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# loading train images\nx_train, x_test, y_train, y_test = read_and_normalize_train_data(img_rows, img_cols, color_type)\n\n# loading validation images\ntest_files, test_targets = read_and_normalize_sampled_test_data(nb_test_samples, img_rows, img_cols, color_type)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T00:11:30.051769Z","iopub.execute_input":"2022-07-09T00:11:30.052299Z","iopub.status.idle":"2022-07-09T00:13:17.982708Z","shell.execute_reply.started":"2022-07-09T00:11:30.052257Z","shell.execute_reply":"2022-07-09T00:13:17.981240Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Loading directory c0\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 1/10 [00:22<03:20, 22.28s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c1\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [00:43<02:53, 21.64s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c2\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [01:03<02:27, 21.13s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c3\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [01:25<02:06, 21.14s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c4\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [01:45<01:44, 20.88s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c5\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [01:47<01:47, 21.57s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/144754731.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading train images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_normalize_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loading validation images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_normalize_sampled_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_test_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/1457274311.py\u001b[0m in \u001b[0;36mread_and_normalize_train_data\u001b[0;34m(img_rows, img_cols, color_type)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mLoad\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#categorical train label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# split into train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/1457274311.py\u001b[0m in \u001b[0;36mload_train\u001b[0;34m(img_rows, img_cols, color_type)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/state-farm-distracted-driver-detection/imgs/train/c'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cv2_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/1457274311.py\u001b[0m in \u001b[0;36mget_cv2_image\u001b[0;34m(path, img_rows, img_cols, color_type)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Loading as Grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcolor_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Loading as color image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"x_train_size = len(x_train)\nx_test_size = len(x_test)\ntest_files_size = len(np.array(glob(os.path.join('../input/state-farm-distracted-driver-detection/imgs/test', '*.jpg'))))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T00:11:25.377053Z","iopub.execute_input":"2022-07-09T00:11:25.377634Z","iopub.status.idle":"2022-07-09T00:11:25.408571Z","shell.execute_reply.started":"2022-07-09T00:11:25.377590Z","shell.execute_reply":"2022-07-09T00:11:25.407319Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/3880393099.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_test_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_files_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/state-farm-distracted-driver-detection/imgs/test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"],"ename":"NameError","evalue":"name 'x_train' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"> ## Statistical numbers about the data","metadata":{}},{"cell_type":"code","source":"print('There are %s total images.' %(x_train_size + x_test_size + test_files_size))\nprint('There are %d total training categories.' %NUMBER_CLASSES )\nprint('There are %d training images.' % x_train_size)\nprint('There are %d validation images.' % x_test_size)\nprint('There are %d test images.'% test_files_size)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T21:02:04.264479Z","iopub.execute_input":"2022-07-08T21:02:04.265090Z","iopub.status.idle":"2022-07-08T21:02:04.270947Z","shell.execute_reply.started":"2022-07-08T21:02:04.265054Z","shell.execute_reply":"2022-07-08T21:02:04.269979Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"There are 102150 total images.\nThere are 10 total training categories.\nThere are 17939 training images.\nThere are 4485 validation images.\nThere are 79726 test images.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> ## Data Visualization","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\npx.histogram(df, x=\"classname\", color=\"classname\", title=\"Number of images by categories \")","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:12.185356Z","iopub.execute_input":"2022-07-08T20:20:12.186062Z","iopub.status.idle":"2022-07-08T20:20:15.626984Z","shell.execute_reply.started":"2022-07-08T20:20:12.186026Z","shell.execute_reply":"2022-07-08T20:20:15.626154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of Images by Drivers / Test Subject","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:15.627987Z","iopub.execute_input":"2022-07-08T20:20:15.628308Z","iopub.status.idle":"2022-07-08T20:20:15.634306Z","shell.execute_reply.started":"2022-07-08T20:20:15.628276Z","shell.execute_reply":"2022-07-08T20:20:15.633224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.save('./x_train.npy',x_train)\n# np.save('./y_train.npy',y_train)\n# np.save('./x_test.npy',x_test)\n# np.save('./y_test.npy',y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:15.636055Z","iopub.execute_input":"2022-07-08T20:20:15.636444Z","iopub.status.idle":"2022-07-08T20:20:15.644016Z","shell.execute_reply.started":"2022-07-08T20:20:15.636344Z","shell.execute_reply":"2022-07-08T20:20:15.643052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fully connected layer","metadata":{}},{"cell_type":"markdown","source":"## Fill this to continue running smoothly, I think","metadata":{}},{"cell_type":"code","source":"# x_train = np.load('../input/dl-project/x_train.npy').astype('float32')/255\n# y_train = np_utils.to_categorical(np.load('../input/dl-project/y_train.npy'))\n# x_val = np.load('../input/dl-project/x_test.npy').astype('float32')/255\n# y_val = np_utils.to_categorical(np.load('../input/dl-project/y_test.npy'))\nno_epoch = 5\nbatch_size = 64\nimg_height = img_rows\nimg_width = img_cols\nchannels = 1","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:31:41.767485Z","iopub.execute_input":"2022-07-08T20:31:41.767824Z","iopub.status.idle":"2022-07-08T20:31:41.774342Z","shell.execute_reply.started":"2022-07-08T20:31:41.76779Z","shell.execute_reply":"2022-07-08T20:31:41.773362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)/255\n# y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n# x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)/255\n# y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:15.655068Z","iopub.execute_input":"2022-07-08T20:20:15.65602Z","iopub.status.idle":"2022-07-08T20:20:15.660941Z","shell.execute_reply.started":"2022-07-08T20:20:15.655986Z","shell.execute_reply":"2022-07-08T20:20:15.659551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building initial model","metadata":{}},{"cell_type":"code","source":"# temp\nx_train_FC = x_train.reshape((x_train_size, img_rows*img_cols*1))\nx_train_FC = x_train_FC.astype('float32')/255\nx_val_FC = x_test.reshape((x_test_size, img_rows*img_cols*1))\nx_val_FC = x_val_FC.astype('float32')/255\ny_val = y_test","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:15.662981Z","iopub.execute_input":"2022-07-08T20:20:15.663963Z","iopub.status.idle":"2022-07-08T20:20:16.156525Z","shell.execute_reply.started":"2022-07-08T20:20:15.663923Z","shell.execute_reply":"2022-07-08T20:20:16.15554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FC_init = Sequential()\n# FC_init.add(Flatten())\nFC_init.add(Dense(512, activation='relu', name='Layer_1', input_shape=(img_width * img_height * channels,)))\nFC_init.add(Dense(256, activation='relu', name='Layer_2'))\nFC_init.add(Dense(128, activation='relu', name='Layer_3'))\nFC_init.add(Dense(10, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:16.159943Z","iopub.execute_input":"2022-07-08T20:20:16.160409Z","iopub.status.idle":"2022-07-08T20:20:18.661934Z","shell.execute_reply.started":"2022-07-08T20:20:16.160367Z","shell.execute_reply":"2022-07-08T20:20:18.660943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = keras.optimizers.Adam(learning_rate=10e-3)\nFC_init.compile(optimizer=opt,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:18.663303Z","iopub.execute_input":"2022-07-08T20:20:18.664118Z","iopub.status.idle":"2022-07-08T20:20:19.010954Z","shell.execute_reply.started":"2022-07-08T20:20:18.664077Z","shell.execute_reply":"2022-07-08T20:20:19.010061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FC_init.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:19.012305Z","iopub.execute_input":"2022-07-08T20:20:19.01283Z","iopub.status.idle":"2022-07-08T20:20:19.021952Z","shell.execute_reply.started":"2022-07-08T20:20:19.012777Z","shell.execute_reply":"2022-07-08T20:20:19.020066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=2,\n    verbose=1,\n    mode='min',\n    baseline=None,\n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:19.023818Z","iopub.execute_input":"2022-07-08T20:20:19.024164Z","iopub.status.idle":"2022-07-08T20:20:19.035256Z","shell.execute_reply.started":"2022-07-08T20:20:19.024128Z","shell.execute_reply":"2022-07-08T20:20:19.034226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"History = FC_init.fit(x_train_FC,y_train, validation_data=(x_val_FC,y_val), verbose = 1, epochs = no_epoch, batch_size = batch_size,callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:19.036899Z","iopub.execute_input":"2022-07-08T20:20:19.037239Z","iopub.status.idle":"2022-07-08T20:20:31.254565Z","shell.execute_reply.started":"2022-07-08T20:20:19.037204Z","shell.execute_reply":"2022-07-08T20:20:31.253596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = History.history['accuracy']\nval_acc = History.history['val_accuracy']\nloss = History.history['loss']\nval_loss = History.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:31.257379Z","iopub.execute_input":"2022-07-08T20:20:31.257859Z","iopub.status.idle":"2022-07-08T20:20:31.654529Z","shell.execute_reply.started":"2022-07-08T20:20:31.257819Z","shell.execute_reply":"2022-07-08T20:20:31.653781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict ","metadata":{}},{"cell_type":"code","source":"test_files.dtype","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:12:58.728673Z","iopub.execute_input":"2022-07-08T20:12:58.729022Z","iopub.status.idle":"2022-07-08T20:12:58.735998Z","shell.execute_reply.started":"2022-07-08T20:12:58.728986Z","shell.execute_reply":"2022-07-08T20:12:58.73484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_files[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-08T12:46:27.333886Z","iopub.execute_input":"2022-07-08T12:46:27.334731Z","iopub.status.idle":"2022-07-08T12:46:27.343738Z","shell.execute_reply.started":"2022-07-08T12:46:27.334695Z","shell.execute_reply":"2022-07-08T12:46:27.342605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.dtype","metadata":{"execution":{"iopub.status.busy":"2022-07-08T12:46:27.345465Z","iopub.execute_input":"2022-07-08T12:46:27.346254Z","iopub.status.idle":"2022-07-08T12:46:27.353407Z","shell.execute_reply.started":"2022-07-08T12:46:27.346076Z","shell.execute_reply":"2022-07-08T12:46:27.352301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_files))\nprint(test_files_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-08T12:46:27.356386Z","iopub.execute_input":"2022-07-08T12:46:27.356662Z","iopub.status.idle":"2022-07-08T12:46:27.362651Z","shell.execute_reply.started":"2022-07-08T12:46:27.356638Z","shell.execute_reply":"2022-07-08T12:46:27.361465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs = test_files.reshape((nb_test_samples, img_rows*img_cols*1))\ntest_imgs = test_imgs.astype('float32')/255\n","metadata":{"execution":{"iopub.status.busy":"2022-07-08T12:46:27.364095Z","iopub.execute_input":"2022-07-08T12:46:27.364778Z","iopub.status.idle":"2022-07-08T12:46:27.374502Z","shell.execute_reply.started":"2022-07-08T12:46:27.36467Z","shell.execute_reply":"2022-07-08T12:46:27.373302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = FC_init.predict(test_imgs)\npred[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-08T12:46:27.376224Z","iopub.execute_input":"2022-07-08T12:46:27.376865Z","iopub.status.idle":"2022-07-08T12:46:27.814157Z","shell.execute_reply.started":"2022-07-08T12:46:27.376827Z","shell.execute_reply":"2022-07-08T12:46:27.813252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FC_init.evaluate(test_imgs, pred)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T12:46:27.815533Z","iopub.execute_input":"2022-07-08T12:46:27.815886Z","iopub.status.idle":"2022-07-08T12:46:27.914182Z","shell.execute_reply.started":"2022-07-08T12:46:27.81585Z","shell.execute_reply":"2022-07-08T12:46:27.913286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate the model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Baseline CNN Model","metadata":{}},{"cell_type":"code","source":"# temp\nx_train_CNN = x_train.reshape((x_train_size, img_rows,img_cols,1))\nx_train_CNN = x_train_CNN.astype('float32')/255\nx_val_CNN = x_test.reshape((x_test_size, img_rows,img_cols,1))\nx_val_CNN = x_val_CNN.astype('float32')/255\ny_val = y_test","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:47.331453Z","iopub.execute_input":"2022-07-08T20:20:47.332067Z","iopub.status.idle":"2022-07-08T20:20:47.826938Z","shell.execute_reply.started":"2022-07-08T20:20:47.332033Z","shell.execute_reply":"2022-07-08T20:20:47.825923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CNN_model = Sequential()\nCNN_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, channels)))\nCNN_model.add(MaxPooling2D((2, 2)))\nCNN_model.add(Conv2D(64, (3, 3), activation='relu'))\nCNN_model.add(MaxPooling2D((2, 2)))\nCNN_model.add(Conv2D(64, (3, 3), activation='relu'))\nCNN_model.add(Flatten())\nCNN_model.add(Dense(64, activation='relu'))\nCNN_model.add(Dense(10, activation='softmax'))\n","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:48.05594Z","iopub.execute_input":"2022-07-08T20:20:48.056243Z","iopub.status.idle":"2022-07-08T20:20:48.117258Z","shell.execute_reply.started":"2022-07-08T20:20:48.056215Z","shell.execute_reply":"2022-07-08T20:20:48.116437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\nCNN_model.compile(optimizer=\"rmsprop\",\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:49.925772Z","iopub.execute_input":"2022-07-08T20:20:49.926639Z","iopub.status.idle":"2022-07-08T20:20:49.939043Z","shell.execute_reply.started":"2022-07-08T20:20:49.92659Z","shell.execute_reply":"2022-07-08T20:20:49.937969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CNN_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:51.487852Z","iopub.execute_input":"2022-07-08T20:20:51.488397Z","iopub.status.idle":"2022-07-08T20:20:51.496298Z","shell.execute_reply.started":"2022-07-08T20:20:51.488358Z","shell.execute_reply":"2022-07-08T20:20:51.49474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"History_CNN = CNN_model.fit(x_train_CNN,y_train, validation_data=(x_val_CNN,y_val), verbose = 1, epochs = no_epoch, batch_size = batch_size,callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:20:53.484293Z","iopub.execute_input":"2022-07-08T20:20:53.484627Z","iopub.status.idle":"2022-07-08T20:21:23.809346Z","shell.execute_reply.started":"2022-07-08T20:20:53.484598Z","shell.execute_reply":"2022-07-08T20:21:23.808416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = History_CNN.history['accuracy']\nval_acc = History_CNN.history['val_accuracy']\nloss = History_CNN.history['loss']\nval_loss = History_CNN.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:21:23.811407Z","iopub.execute_input":"2022-07-08T20:21:23.81179Z","iopub.status.idle":"2022-07-08T20:21:24.174314Z","shell.execute_reply.started":"2022-07-08T20:21:23.811751Z","shell.execute_reply":"2022-07-08T20:21:24.173411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs = test_files.reshape((nb_test_samples, img_rows,img_cols,1))\ntest_imgs = test_imgs.astype('float32')/255\n","metadata":{"execution":{"iopub.status.busy":"2022-07-08T20:13:43.519035Z","iopub.execute_input":"2022-07-08T20:13:43.519338Z","iopub.status.idle":"2022-07-08T20:13:43.52748Z","shell.execute_reply.started":"2022-07-08T20:13:43.519312Z","shell.execute_reply":"2022-07-08T20:13:43.526287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = CNN_model.predict(test_imgs)\npred[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"# Version 01\n\n# Traindatagen = ImageDataGenerator(\n#       featurewise_center = True,\n#       featurewise_std_normalization = True,\n#       rescale = 1.0/255,\n#       rotation_range=40,\n#       width_shift_range=0.2,\n#       height_shift_range=0.2,\n#       shear_range=0.2,\n#       zoom_range=0.2,\n#       horizontal_flip=True,\n#       vertical_flip=False,\n#       fill_mode='nearest')\n\n\n# Valdatagen =  ImageDataGenerator(featurewise_center = True, \n#                                  featurewise_std_normalization = True, \n#                                  rescale=1.0/ 255, \n#                                  validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T21:02:37.841178Z","iopub.execute_input":"2022-07-08T21:02:37.841663Z","iopub.status.idle":"2022-07-08T21:02:37.849818Z","shell.execute_reply.started":"2022-07-08T21:02:37.841620Z","shell.execute_reply":"2022-07-08T21:02:37.848857Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Version 02\n\nTraindatagen = ImageDataGenerator(\n      featurewise_center = True,\n      featurewise_std_normalization = True,\n      rescale = 1.0/255,\n      rotation_range=20,\n      shear_range=0.2,\n      horizontal_flip=True,\n      vertical_flip=False,\n      fill_mode='nearest')\n\n\nValdatagen =  ImageDataGenerator(featurewise_center = True, \n                                 featurewise_std_normalization = True, \n                                 rescale=1.0/ 255, \n                                 validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T22:58:08.675896Z","iopub.execute_input":"2022-07-08T22:58:08.676921Z","iopub.status.idle":"2022-07-08T22:58:08.684840Z","shell.execute_reply.started":"2022-07-08T22:58:08.676883Z","shell.execute_reply":"2022-07-08T22:58:08.683723Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Transfer learning using VGG16","metadata":{}},{"cell_type":"code","source":"# loading train images\nx_train_vgg, x_test_vgg, y_train_vgg, y_test_vgg = read_and_normalize_train_data(img_rows, img_cols, 3)\n\n# loading validation images\n# test_files, test_targets = read_and_normalize_sampled_test_data(nb_test_samples, img_rows, img_cols, 3)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T22:59:01.924426Z","iopub.execute_input":"2022-07-08T22:59:01.925020Z","iopub.status.idle":"2022-07-08T23:02:44.126173Z","shell.execute_reply.started":"2022-07-08T22:59:01.924988Z","shell.execute_reply":"2022-07-08T23:02:44.125207Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Loading directory c0\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 1/10 [00:24<03:43, 24.88s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c1\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [00:47<03:10, 23.84s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c2\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [01:10<02:43, 23.35s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c3\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [01:33<02:19, 23.21s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c4\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [01:56<01:55, 23.03s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c5\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [02:19<01:31, 22.99s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c6\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [02:42<01:08, 22.91s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c7\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [03:02<00:44, 22.02s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c8\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [03:20<00:20, 20.94s/it]","output_type":"stream"},{"name":"stdout","text":"Loading directory c9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [03:41<00:00, 22.18s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_generator = Traindatagen.flow(x_train_vgg, y_train_vgg, batch_size = 256)\nval_generator = Valdatagen.flow(x_test_vgg, y_test_vgg, batch_size = 64)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:03:11.142029Z","iopub.execute_input":"2022-07-08T23:03:11.142979Z","iopub.status.idle":"2022-07-08T23:03:12.558611Z","shell.execute_reply.started":"2022-07-08T23:03:11.142934Z","shell.execute_reply":"2022-07-08T23:03:12.557627Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"epochs = 20\ntrain_bs = 256\nvaldi_bs = 64","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:03:16.015896Z","iopub.execute_input":"2022-07-08T23:03:16.016290Z","iopub.status.idle":"2022-07-08T23:03:16.023759Z","shell.execute_reply.started":"2022-07-08T23:03:16.016256Z","shell.execute_reply":"2022-07-08T23:03:16.020186Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\n\ndef classificationModel():\n    inp = keras.layers.Input(shape=(128, 128, 3))\n    vgg = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_tensor = inp,\n                  input_shape=(128, 128, 3))\n    vgg.trainable = False\n    \n    x = vgg.get_layer('block5_pool').output\n    x = tf.keras.layers.Flatten()(x)\n    # x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(256, activation='relu')(x)\n    output = keras.layers.Dense(10, activation='softmax')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs=output)\n\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:04:49.299775Z","iopub.execute_input":"2022-07-08T23:04:49.300168Z","iopub.status.idle":"2022-07-08T23:04:49.309728Z","shell.execute_reply.started":"2022-07-08T23:04:49.300136Z","shell.execute_reply":"2022-07-08T23:04:49.306936Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nmodel = classificationModel()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:04:50.550227Z","iopub.execute_input":"2022-07-08T23:04:50.551304Z","iopub.status.idle":"2022-07-08T23:04:53.876347Z","shell.execute_reply.started":"2022-07-08T23:04:50.551250Z","shell.execute_reply":"2022-07-08T23:04:53.875353Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"2022-07-08 23:04:50.636516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:50.763731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:50.764459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:50.765767: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-08 23:04:50.766087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:50.766910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:50.767716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:52.783191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:52.784056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:52.784718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-08 23:04:52.785369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58892288/58889256 [==============================] - 0s 0us/step\n58900480/58889256 [==============================] - 0s 0us/step\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 8192)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               2097408   \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                2570      \n=================================================================\nTotal params: 16,814,666\nTrainable params: 2,099,978\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n# model.compile(optimizer='rmsprop',\n#                 loss='categorical_crossentropy',\n#                 metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-08T21:13:45.885540Z","iopub.execute_input":"2022-07-08T21:13:45.886145Z","iopub.status.idle":"2022-07-08T21:13:45.899544Z","shell.execute_reply.started":"2022-07-08T21:13:45.886110Z","shell.execute_reply":"2022-07-08T21:13:45.898592Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=opt,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:05:07.866960Z","iopub.execute_input":"2022-07-08T23:05:07.867744Z","iopub.status.idle":"2022-07-08T23:05:07.892271Z","shell.execute_reply.started":"2022-07-08T23:05:07.867694Z","shell.execute_reply":"2022-07-08T23:05:07.891116Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0001,\n    patience=3,\n    verbose=1,\n    mode='min',\n    baseline=None,\n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:05:14.158280Z","iopub.execute_input":"2022-07-08T23:05:14.159341Z","iopub.status.idle":"2022-07-08T23:05:14.165078Z","shell.execute_reply.started":"2022-07-08T23:05:14.159264Z","shell.execute_reply":"2022-07-08T23:05:14.163878Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## CNN Visualization","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:05:48.828516Z","iopub.execute_input":"2022-07-08T23:05:48.829434Z","iopub.status.idle":"2022-07-08T23:05:59.493441Z","shell.execute_reply.started":"2022-07-08T23:05:48.829397Z","shell.execute_reply":"2022-07-08T23:05:59.492146Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom wandb.keras import WandbCallback\n\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:05:59.495748Z","iopub.execute_input":"2022-07-08T23:05:59.496463Z","iopub.status.idle":"2022-07-08T23:06:12.355939Z","shell.execute_reply.started":"2022-07-08T23:05:59.496416Z","shell.execute_reply":"2022-07-08T23:06:12.354723Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"class GradCAM:\n    \"\"\"\n    Reference:\n        https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\n    \"\"\"\n\n    def __init__(self, model, layerName):\n        self.model = model\n        self.layerName = layerName\n\n        self.gradModel = tf.keras.models.Model(inputs=[self.model.inputs],\n                                               outputs=[self.model.get_layer(self.layerName).output, self.model.output])\n\n    def compute_heatmap(self, image, classIdx, eps=1e-8):\n        with tf.GradientTape() as tape:\n            tape.watch(self.gradModel.get_layer(self.layerName).variables)\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = self.gradModel(inputs)\n\n            if len(predictions) == 1:\n                # Binary Classification\n                loss = predictions[0]\n            else:\n                loss = predictions[:, classIdx]\n\n        grads = tape.gradient(loss, convOutputs)\n\n        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n        castGrads = tf.cast(grads > 0, \"float32\")\n        guidedGrads = castConvOutputs * castGrads * grads\n\n        convOutputs = convOutputs[0]\n        guidedGrads = guidedGrads[0]\n\n        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n\n        (w, h) = (image.shape[2], image.shape[1])\n        heatmap = cv2.resize(cam.numpy(), (w, h))\n\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n        heatmap = (heatmap * 255).astype(\"uint8\")\n\n        return heatmap\n\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_HOT):\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n\n        return (heatmap, output)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:06:16.118371Z","iopub.execute_input":"2022-07-08T23:06:16.119243Z","iopub.status.idle":"2022-07-08T23:06:16.135050Z","shell.execute_reply.started":"2022-07-08T23:06:16.119198Z","shell.execute_reply":"2022-07-08T23:06:16.133866Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class GRADCamLogger(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data, layer_name):\n      super(GRADCamLogger, self).__init__()\n      self.validation_data = validation_data\n      self.layer_name = layer_name\n\n    def on_epoch_end(self, logs, epoch):\n      images = []\n      grad_cam = []\n\n      ## Initialize GRADCam Class\n      cam = GradCAM(model, self.layer_name)\n\n      for image in self.validation_data:\n        image = np.expand_dims(image, 0)\n        pred = model.predict(image)\n        classIDx = np.argmax(pred[0])\n  \n        ## Compute Heatmap\n        heatmap = cam.compute_heatmap(image, classIDx)\n        \n        image = image.reshape(image.shape[1:])\n        image = image*255\n        image = image.astype(np.uint8)\n\n        ## Overlay heatmap on original image\n        heatmap = cv2.resize(heatmap, (image.shape[0],image.shape[1]))\n        (heatmap, output) = cam.overlay_heatmap(heatmap, image, alpha=0.5)\n\n        images.append(image)\n        grad_cam.append(output)\n\n      wandb.log({\"images\": [wandb.Image(image)\n                            for image in images]})\n      wandb.log({\"gradcam\": [wandb.Image(cam)\n                            for cam in grad_cam]})","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:06:17.061210Z","iopub.execute_input":"2022-07-08T23:06:17.061962Z","iopub.status.idle":"2022-07-08T23:06:17.072171Z","shell.execute_reply.started":"2022-07-08T23:06:17.061923Z","shell.execute_reply":"2022-07-08T23:06:17.071090Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"## Prepare sample images to run your GradCam on. \nsample_images, sample_labels = val_generator[20]\nsample_images.shape, sample_labels.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:06:34.469468Z","iopub.execute_input":"2022-07-08T23:06:34.469856Z","iopub.status.idle":"2022-07-08T23:06:34.491058Z","shell.execute_reply.started":"2022-07-08T23:06:34.469824Z","shell.execute_reply":"2022-07-08T23:06:34.489964Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n  warnings.warn('This ImageDataGenerator specifies '\n/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n  warnings.warn('This ImageDataGenerator specifies '\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"((64, 128, 128, 3), (64, 10))"},"metadata":{}}]},{"cell_type":"code","source":"# wandb.init(entity='ayush-thakur', project='interpretability')\nwandb.init(project=\"test\", entity=\"team-7\")","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:06:38.952338Z","iopub.execute_input":"2022-07-08T23:06:38.953004Z","iopub.status.idle":"2022-07-08T23:06:41.920556Z","shell.execute_reply.started":"2022-07-08T23:06:38.952966Z","shell.execute_reply":"2022-07-08T23:06:41.919554Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mz1z\u001b[0m (\u001b[33mteam-7\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.12.21 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.18"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220708_230639-dqudlti3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/team-7/test/runs/dqudlti3\" target=\"_blank\">floral-snowflake-4</a></strong> to <a href=\"https://wandb.ai/team-7/test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/team-7/test/runs/dqudlti3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f1f704f37d0>"},"metadata":{}}]},{"cell_type":"markdown","source":"> ## Accuracy <br>\n- The transfer Learning using VGG16 conv_base and the augmented data reached an **accuracy** of **98.42%**","metadata":{}},{"cell_type":"code","source":"# History_Rs = resnetRs.fit(train_generator,\n#          validation_data=val_generator,\n#          steps_per_epoch=len(x_train_rs) // train_bs, epochs=epochs,\n#           validation_steps =len(x_test_rs)//valdi_bs, verbose = 1)\n\n\nhistory = model.fit(train_generator,\n                          validation_data=val_generator,\n                          steps_per_epoch=len(x_train_vgg) // train_bs, \n                          epochs=epochs,\n                          validation_steps =len(x_test_vgg)//valdi_bs, \n                          verbose = 1,\n                          callbacks=[WandbCallback(data_type=\"image\", validation_data=(sample_images, sample_labels)),\n                                     GRADCamLogger(sample_images, layer_name='block5_conv3'),\n                                     early_stopping])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-08T23:42:54.566855Z","iopub.execute_input":"2022-07-08T23:42:54.567318Z","iopub.status.idle":"2022-07-09T00:00:06.953660Z","shell.execute_reply.started":"2022-07-08T23:42:54.567279Z","shell.execute_reply":"2022-07-09T00:00:06.951393Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1/20\n70/70 [==============================] - 70s 1s/step - loss: 0.1018 - accuracy: 0.9820 - val_loss: 0.1039 - val_accuracy: 0.9759\nEpoch 2/20\n70/70 [==============================] - 70s 993ms/step - loss: 0.1008 - accuracy: 0.9815 - val_loss: 0.1040 - val_accuracy: 0.9754\nEpoch 3/20\n70/70 [==============================] - 71s 1s/step - loss: 0.0942 - accuracy: 0.9827 - val_loss: 0.0951 - val_accuracy: 0.9781\nEpoch 4/20\n70/70 [==============================] - 70s 1s/step - loss: 0.0875 - accuracy: 0.9848 - val_loss: 0.0953 - val_accuracy: 0.9772\nEpoch 5/20\n70/70 [==============================] - 71s 1s/step - loss: 0.0842 - accuracy: 0.9847 - val_loss: 0.0927 - val_accuracy: 0.9786\nEpoch 6/20\n70/70 [==============================] - 71s 1s/step - loss: 0.0822 - accuracy: 0.9843 - val_loss: 0.0835 - val_accuracy: 0.9806\nEpoch 7/20\n70/70 [==============================] - 70s 1s/step - loss: 0.0778 - accuracy: 0.9857 - val_loss: 0.0810 - val_accuracy: 0.9815\nEpoch 8/20\n70/70 [==============================] - 71s 1s/step - loss: 0.0736 - accuracy: 0.9875 - val_loss: 0.0791 - val_accuracy: 0.9817\nEpoch 9/20\n70/70 [==============================] - 73s 1s/step - loss: 0.0737 - accuracy: 0.9864 - val_loss: 0.0730 - val_accuracy: 0.9842\nEpoch 10/20\n70/70 [==============================] - 72s 1s/step - loss: 0.0655 - accuracy: 0.9891 - val_loss: 0.0755 - val_accuracy: 0.9812\nEpoch 11/20\n70/70 [==============================] - 71s 1s/step - loss: 0.0663 - accuracy: 0.9885 - val_loss: 0.0758 - val_accuracy: 0.9810\nEpoch 12/20\n70/70 [==============================] - 71s 1s/step - loss: 0.0672 - accuracy: 0.9878 - val_loss: 0.0744 - val_accuracy: 0.9808\nRestoring model weights from the end of the best epoch.\nEpoch 00012: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('./vgg_tl_model.h5')","metadata":{"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"new_model = tf.keras.models.load_model('./my_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-07-08T15:06:00.280427Z","iopub.execute_input":"2022-07-08T15:06:00.280931Z","iopub.status.idle":"2022-07-08T15:06:00.645273Z","shell.execute_reply.started":"2022-07-08T15:06:00.280886Z","shell.execute_reply":"2022-07-08T15:06:00.644115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = History_Rs_2.history['accuracy']\nval_acc = History_Rs_2.history['val_accuracy']\nloss = History_Rs_2.history['loss']\nval_loss = History_Rs_2.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T14:50:41.49143Z","iopub.execute_input":"2022-07-08T14:50:41.491928Z","iopub.status.idle":"2022-07-08T14:50:41.967945Z","shell.execute_reply.started":"2022-07-08T14:50:41.491885Z","shell.execute_reply":"2022-07-08T14:50:41.967068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}