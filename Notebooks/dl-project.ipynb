{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Team: <br>\n1. Dina Zakria \n2. Ahmed Sameh\n3. Abdelrhman Amr","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-09T14:25:15.179853Z","iopub.execute_input":"2022-07-09T14:25:15.180813Z","iopub.status.idle":"2022-07-09T14:25:15.203549Z","shell.execute_reply.started":"2022-07-09T14:25:15.180714Z","shell.execute_reply":"2022-07-09T14:25:15.202733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Imports\nimport cv2\nimport os\nfrom tqdm import tqdm\nfrom glob import glob\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:25:40.29872Z","iopub.execute_input":"2022-07-09T14:25:40.299064Z","iopub.status.idle":"2022-07-09T14:25:45.643798Z","shell.execute_reply.started":"2022-07-09T14:25:40.299036Z","shell.execute_reply":"2022-07-09T14:25:45.642737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/state-farm-distracted-driver-detection/driver_imgs_list.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:25:45.645883Z","iopub.execute_input":"2022-07-09T14:25:45.64638Z","iopub.status.idle":"2022-07-09T14:25:45.692159Z","shell.execute_reply.started":"2022-07-09T14:25:45.646352Z","shell.execute_reply":"2022-07-09T14:25:45.691127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group bt Drivers/ Test Subjects\nby_drivers = df.groupby('subject')\n\nunique_drivers = by_drivers.groups.keys()\n\nprint(\"There are: \", len(unique_drivers), \" unique drivers\")\nprint('These are the numbers of each test subject: \\n',round(by_drivers.count()['classname']))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:09:25.189933Z","iopub.execute_input":"2022-07-09T14:09:25.190477Z","iopub.status.idle":"2022-07-09T14:09:25.217961Z","shell.execute_reply.started":"2022-07-09T14:09:25.190406Z","shell.execute_reply":"2022-07-09T14:09:25.216658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the number of classes to be classified.\nNUMBER_CLASSES = 10","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:25:45.69395Z","iopub.execute_input":"2022-07-09T14:25:45.694326Z","iopub.status.idle":"2022-07-09T14:25:45.699612Z","shell.execute_reply.started":"2022-07-09T14:25:45.694287Z","shell.execute_reply":"2022-07-09T14:25:45.698557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Helper Functions","metadata":{}},{"cell_type":"code","source":"# Read with opencv\ndef get_cv2_image(path, img_rows, img_cols, color_type=3):\n    \"\"\"\n    Function that return an opencv image from the path and the right number of dimension\n    \"\"\"\n    if color_type == 1: # Loading as Grayscale image\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    elif color_type == 3: # Loading as color image\n        img = cv2.imread(path, cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Converts to RGB\n\n    img = cv2.resize(img, (img_rows, img_cols)) # Reduce size\n    return img\n\n# Loading Training dataset\ndef load_train(img_rows, img_cols, color_type=3):\n    \"\"\"\n    Return train images and train labels from the original path\n    \"\"\"\n    train_images = [] \n    train_labels = []\n    # Loop over the training folder \n    for classed in tqdm(range(NUMBER_CLASSES)):\n        print('Loading directory c{}'.format(classed))\n        files = glob(os.path.join('../input/state-farm-distracted-driver-detection/imgs/train/c' + str(classed), '*.jpg'))\n        for file in files:\n            img = get_cv2_image(file, img_rows, img_cols, color_type)\n            train_images.append(img)\n            train_labels.append(classed)\n    return train_images, train_labels \n\ndef read_and_normalize_train_data(img_rows, img_cols, color_type):\n    \"\"\"\n    Load + categorical + split\n    \"\"\"\n    X, labels = load_train(img_rows, img_cols, color_type)\n    y = np_utils.to_categorical(labels, 10) #categorical train label\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split into train and test\n    x_train = np.array(x_train, dtype=np.uint8).reshape(-1,img_rows,img_cols,color_type)\n    x_test = np.array(x_test, dtype=np.uint8).reshape(-1,img_rows,img_cols,color_type)\n    \n    return x_train, x_test, y_train, y_test\n\n# Loading validation dataset\ndef load_test(size=200000, img_rows=64, img_cols=64, color_type=3):\n    \"\"\"\n    Same as above but for validation dataset\n    \"\"\"\n    path = os.path.join('../input/state-farm-distracted-driver-detection/imgs/test', '*.jpg')\n    files = sorted(glob(path))\n    X_test, X_test_id = [], []\n    total = 0\n    files_size = len(files)\n    for file in tqdm(files):\n        if total >= size or total >= files_size:\n            break\n        file_base = os.path.basename(file)\n        img = get_cv2_image(file, img_rows, img_cols, color_type)\n        X_test.append(img)\n        X_test_id.append(file_base)\n        total += 1\n    return X_test, X_test_id\n\ndef read_and_normalize_sampled_test_data(size, img_rows, img_cols, color_type=3):\n    test_data, test_ids = load_test(size, img_rows, img_cols, color_type)   \n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.reshape(-1,img_rows,img_cols,color_type)\n    return test_data, test_ids","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:25:47.079916Z","iopub.execute_input":"2022-07-09T14:25:47.080284Z","iopub.status.idle":"2022-07-09T14:25:47.09953Z","shell.execute_reply.started":"2022-07-09T14:25:47.080252Z","shell.execute_reply":"2022-07-09T14:25:47.098374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dimension of images\nimg_rows = 128 \nimg_cols = 128\n\ncolor_type = 1 # grey\nnb_test_samples = 200","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:25:48.507849Z","iopub.execute_input":"2022-07-09T14:25:48.508301Z","iopub.status.idle":"2022-07-09T14:25:48.51423Z","shell.execute_reply.started":"2022-07-09T14:25:48.508262Z","shell.execute_reply":"2022-07-09T14:25:48.513106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading train images\nx_train, x_test, y_train, y_test = read_and_normalize_train_data(img_rows, img_cols, color_type)\n\n# loading validation images\ntest_files, test_targets = read_and_normalize_sampled_test_data(nb_test_samples, img_rows, img_cols, color_type)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:09:25.27179Z","iopub.execute_input":"2022-07-09T14:09:25.272327Z","iopub.status.idle":"2022-07-09T14:12:42.55116Z","shell.execute_reply.started":"2022-07-09T14:09:25.272281Z","shell.execute_reply":"2022-07-09T14:12:42.549814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"x_train_size = len(x_train)\nx_test_size = len(x_test)\ntest_files_size = len(np.array(glob(os.path.join('../input/state-farm-distracted-driver-detection/imgs/test', '*.jpg'))))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:42.55667Z","iopub.execute_input":"2022-07-09T14:12:42.559612Z","iopub.status.idle":"2022-07-09T14:12:43.057099Z","shell.execute_reply.started":"2022-07-09T14:12:42.559565Z","shell.execute_reply":"2022-07-09T14:12:43.055688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Statistical numbers about the data","metadata":{}},{"cell_type":"code","source":"print('There are %s total images.' %(x_train_size + x_test_size + test_files_size))\nprint('There are %d total training categories.' %NUMBER_CLASSES )\nprint('There are %d training images.' % x_train_size)\nprint('There are %d validation images.' % x_test_size)\nprint('There are %d test images.'% test_files_size)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:43.05866Z","iopub.execute_input":"2022-07-09T14:12:43.059123Z","iopub.status.idle":"2022-07-09T14:12:43.067805Z","shell.execute_reply.started":"2022-07-09T14:12:43.059077Z","shell.execute_reply":"2022-07-09T14:12:43.066091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Data Visualization","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\npx.histogram(df, x=\"classname\", color=\"classname\", title=\"Number of images by categories \")","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:43.0736Z","iopub.execute_input":"2022-07-09T14:12:43.074563Z","iopub.status.idle":"2022-07-09T14:12:47.337852Z","shell.execute_reply.started":"2022-07-09T14:12:43.074518Z","shell.execute_reply":"2022-07-09T14:12:47.336465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Comment:** As we can see from the figure above, the classes are well balanced.","metadata":{}},{"cell_type":"code","source":"# Number of Images by Drivers / Test Subject\n\ndrivers_id = pd.DataFrame((df['subject'].value_counts()).reset_index())\ndrivers_id.columns = ['driver_id', 'Counts']\npx.histogram(drivers_id, x=\"driver_id\",y=\"Counts\" ,color=\"driver_id\", title=\"Number of images by subjects \")","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:47.339122Z","iopub.execute_input":"2022-07-09T14:12:47.339726Z","iopub.status.idle":"2022-07-09T14:12:47.683336Z","shell.execute_reply.started":"2022-07-09T14:12:47.339671Z","shell.execute_reply":"2022-07-09T14:12:47.682192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.save('./x_train.npy',x_train)\n# np.save('./y_train.npy',y_train)\n# np.save('./x_test.npy',x_test)\n# np.save('./y_test.npy',y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:47.685086Z","iopub.execute_input":"2022-07-09T14:12:47.691651Z","iopub.status.idle":"2022-07-09T14:12:47.69678Z","shell.execute_reply.started":"2022-07-09T14:12:47.691607Z","shell.execute_reply":"2022-07-09T14:12:47.695612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fully connected layer","metadata":{}},{"cell_type":"code","source":"# x_train = np.load('../input/dl-project/x_train.npy').astype('float32')/255\n# y_train = np_utils.to_categorical(np.load('../input/dl-project/y_train.npy'))\n# x_val = np.load('../input/dl-project/x_test.npy').astype('float32')/255\n# y_val = np_utils.to_categorical(np.load('../input/dl-project/y_test.npy'))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:47.698922Z","iopub.execute_input":"2022-07-09T14:12:47.699427Z","iopub.status.idle":"2022-07-09T14:12:47.714219Z","shell.execute_reply.started":"2022-07-09T14:12:47.699383Z","shell.execute_reply":"2022-07-09T14:12:47.712841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)/255\n# y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n# x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)/255\n# y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:47.715656Z","iopub.execute_input":"2022-07-09T14:12:47.716839Z","iopub.status.idle":"2022-07-09T14:12:47.726252Z","shell.execute_reply.started":"2022-07-09T14:12:47.716798Z","shell.execute_reply":"2022-07-09T14:12:47.724942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_epoch = 20\nbatch_size = 64\nimg_height = img_rows\nimg_width = img_cols\nchannels = 1","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:26:15.474548Z","iopub.execute_input":"2022-07-09T14:26:15.475188Z","iopub.status.idle":"2022-07-09T14:26:15.479971Z","shell.execute_reply.started":"2022-07-09T14:26:15.475151Z","shell.execute_reply":"2022-07-09T14:26:15.479039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Building initial model","metadata":{}},{"cell_type":"code","source":"# temp\nx_train_FC = x_train.reshape((x_train_size, img_rows*img_cols*1))\nx_train_FC = x_train_FC.astype('float32')/255\nx_val_FC = x_test.reshape((x_test_size, img_rows*img_cols*1))\nx_val_FC = x_val_FC.astype('float32')/255\ny_val = y_test","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:47.740435Z","iopub.execute_input":"2022-07-09T14:12:47.742375Z","iopub.status.idle":"2022-07-09T14:12:48.431865Z","shell.execute_reply.started":"2022-07-09T14:12:47.742334Z","shell.execute_reply":"2022-07-09T14:12:48.430622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FC_init1 = Sequential()\n# FC_init.add(Flatten())\nFC_init1.add(Dense(512, activation='relu', name='Layer_1', input_shape=(img_width * img_height * channels,)))\nFC_init1.add(Dense(256, activation='relu', name='Layer_2'))\nFC_init1.add(Dense(128, activation='relu', name='Layer_3'))\nFC_init1.add(Dense(10, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:48.433782Z","iopub.execute_input":"2022-07-09T14:12:48.434183Z","iopub.status.idle":"2022-07-09T14:12:51.69047Z","shell.execute_reply.started":"2022-07-09T14:12:48.434142Z","shell.execute_reply":"2022-07-09T14:12:51.68845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = keras.optimizers.Adam(learning_rate=10e-3)\nFC_init1.compile(optimizer=opt,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:51.692333Z","iopub.execute_input":"2022-07-09T14:12:51.693526Z","iopub.status.idle":"2022-07-09T14:12:52.128036Z","shell.execute_reply.started":"2022-07-09T14:12:51.693468Z","shell.execute_reply":"2022-07-09T14:12:52.126751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FC_init1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:52.129831Z","iopub.execute_input":"2022-07-09T14:12:52.130295Z","iopub.status.idle":"2022-07-09T14:12:52.143457Z","shell.execute_reply.started":"2022-07-09T14:12:52.130249Z","shell.execute_reply":"2022-07-09T14:12:52.140557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.001,\n    patience=3,\n    verbose=1,\n    mode='min',\n    baseline=None,\n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:52.145488Z","iopub.execute_input":"2022-07-09T14:12:52.146487Z","iopub.status.idle":"2022-07-09T14:12:52.159813Z","shell.execute_reply.started":"2022-07-09T14:12:52.146442Z","shell.execute_reply":"2022-07-09T14:12:52.15842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"History = FC_init1.fit(x_train_FC,y_train, validation_data=(x_val_FC,y_val), verbose = 1, epochs = no_epoch, batch_size = batch_size,callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:12:52.161812Z","iopub.execute_input":"2022-07-09T14:12:52.16278Z","iopub.status.idle":"2022-07-09T14:13:24.351611Z","shell.execute_reply.started":"2022-07-09T14:12:52.162735Z","shell.execute_reply":"2022-07-09T14:13:24.35028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = History.history['accuracy']\nval_acc = History.history['val_accuracy']\nloss = History.history['loss']\nval_loss = History.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:39.338258Z","iopub.execute_input":"2022-07-09T14:13:39.338965Z","iopub.status.idle":"2022-07-09T14:13:39.830548Z","shell.execute_reply.started":"2022-07-09T14:13:39.33893Z","shell.execute_reply":"2022-07-09T14:13:39.829344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict ","metadata":{}},{"cell_type":"code","source":"test_files.dtype","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:44.497098Z","iopub.execute_input":"2022-07-09T14:13:44.497637Z","iopub.status.idle":"2022-07-09T14:13:44.506704Z","shell.execute_reply.started":"2022-07-09T14:13:44.497599Z","shell.execute_reply":"2022-07-09T14:13:44.505269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_files[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:44.668249Z","iopub.execute_input":"2022-07-09T14:13:44.670732Z","iopub.status.idle":"2022-07-09T14:13:44.680222Z","shell.execute_reply.started":"2022-07-09T14:13:44.670663Z","shell.execute_reply":"2022-07-09T14:13:44.678553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.dtype","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:44.892558Z","iopub.execute_input":"2022-07-09T14:13:44.892973Z","iopub.status.idle":"2022-07-09T14:13:44.903369Z","shell.execute_reply.started":"2022-07-09T14:13:44.89294Z","shell.execute_reply":"2022-07-09T14:13:44.901749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_files))\nprint(test_files_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:45.117388Z","iopub.execute_input":"2022-07-09T14:13:45.117821Z","iopub.status.idle":"2022-07-09T14:13:45.126255Z","shell.execute_reply.started":"2022-07-09T14:13:45.11779Z","shell.execute_reply":"2022-07-09T14:13:45.124514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs = test_files.reshape((nb_test_samples, img_rows*img_cols*1))\ntest_imgs = test_imgs.astype('float32')/255\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:48.187545Z","iopub.execute_input":"2022-07-09T14:13:48.188619Z","iopub.status.idle":"2022-07-09T14:13:48.201729Z","shell.execute_reply.started":"2022-07-09T14:13:48.188573Z","shell.execute_reply":"2022-07-09T14:13:48.200485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = FC_init1.predict(test_imgs)\npred[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:48.945121Z","iopub.execute_input":"2022-07-09T14:13:48.945572Z","iopub.status.idle":"2022-07-09T14:13:49.112006Z","shell.execute_reply.started":"2022-07-09T14:13:48.945542Z","shell.execute_reply":"2022-07-09T14:13:49.110611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Baseline CNN Model","metadata":{}},{"cell_type":"code","source":"# temp\nx_train_CNN = x_train.reshape((x_train_size, img_rows,img_cols,1))\nx_train_CNN = x_train_CNN.astype('float32')/255\nx_val_CNN = x_test.reshape((x_test_size, img_rows,img_cols,1))\nx_val_CNN = x_val_CNN.astype('float32')/255\ny_val = y_test","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:53.039622Z","iopub.execute_input":"2022-07-09T14:13:53.040588Z","iopub.status.idle":"2022-07-09T14:13:53.745675Z","shell.execute_reply.started":"2022-07-09T14:13:53.040538Z","shell.execute_reply":"2022-07-09T14:13:53.744432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CNN_model = Sequential()\nCNN_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, channels)))\nCNN_model.add(MaxPooling2D((2, 2)))\nCNN_model.add(Conv2D(64, (3, 3), activation='relu'))\nCNN_model.add(MaxPooling2D((2, 2)))\nCNN_model.add(Conv2D(64, (3, 3), activation='relu'))\nCNN_model.add(Flatten())\nCNN_model.add(Dense(64, activation='relu'))\nCNN_model.add(Dense(10, activation='softmax'))\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:53.761979Z","iopub.execute_input":"2022-07-09T14:13:53.763034Z","iopub.status.idle":"2022-07-09T14:13:53.864992Z","shell.execute_reply.started":"2022-07-09T14:13:53.762988Z","shell.execute_reply":"2022-07-09T14:13:53.863861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\nCNN_model.compile(optimizer=adam,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:55.006677Z","iopub.execute_input":"2022-07-09T14:13:55.00707Z","iopub.status.idle":"2022-07-09T14:13:55.023661Z","shell.execute_reply.started":"2022-07-09T14:13:55.007039Z","shell.execute_reply":"2022-07-09T14:13:55.022269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CNN_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:55.934553Z","iopub.execute_input":"2022-07-09T14:13:55.934939Z","iopub.status.idle":"2022-07-09T14:13:55.943132Z","shell.execute_reply.started":"2022-07-09T14:13:55.93491Z","shell.execute_reply":"2022-07-09T14:13:55.941704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"History_CNN = CNN_model.fit(x_train_CNN,y_train, validation_data=(x_val_CNN,y_val), verbose = 1, epochs = no_epoch, batch_size = batch_size,callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:13:59.656152Z","iopub.execute_input":"2022-07-09T14:13:59.656799Z","iopub.status.idle":"2022-07-09T14:15:23.890677Z","shell.execute_reply.started":"2022-07-09T14:13:59.656755Z","shell.execute_reply":"2022-07-09T14:15:23.889213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = History_CNN.history['accuracy']\nval_acc = History_CNN.history['val_accuracy']\nloss = History_CNN.history['loss']\nval_loss = History_CNN.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:15:23.893296Z","iopub.execute_input":"2022-07-09T14:15:23.893638Z","iopub.status.idle":"2022-07-09T14:15:24.500216Z","shell.execute_reply.started":"2022-07-09T14:15:23.893608Z","shell.execute_reply":"2022-07-09T14:15:24.498842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs = test_files.reshape((nb_test_samples, img_rows,img_cols,1))\ntest_imgs = test_imgs.astype('float32')/255\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:15:24.502145Z","iopub.execute_input":"2022-07-09T14:15:24.502626Z","iopub.status.idle":"2022-07-09T14:15:24.515551Z","shell.execute_reply.started":"2022-07-09T14:15:24.502595Z","shell.execute_reply":"2022-07-09T14:15:24.513417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = CNN_model.predict(test_imgs)\npred[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:15:24.519359Z","iopub.execute_input":"2022-07-09T14:15:24.521667Z","iopub.status.idle":"2022-07-09T14:15:24.725678Z","shell.execute_reply.started":"2022-07-09T14:15:24.521623Z","shell.execute_reply":"2022-07-09T14:15:24.724513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"# Version 01\n\nTraindatagen = ImageDataGenerator(\n      featurewise_center = True,\n      featurewise_std_normalization = True,\n      rescale = 1.0/255,\n      rotation_range=20,\n      shear_range=0.2,\n      horizontal_flip=True,\n      vertical_flip=False,\n      fill_mode='nearest')\n\n\nValdatagen =  ImageDataGenerator(featurewise_center = True, \n                                 featurewise_std_normalization = True, \n                                 rescale=1.0/ 255, \n                                 validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:07.647354Z","iopub.execute_input":"2022-07-09T14:33:07.647708Z","iopub.status.idle":"2022-07-09T14:33:07.653976Z","shell.execute_reply.started":"2022-07-09T14:33:07.647658Z","shell.execute_reply":"2022-07-09T14:33:07.65294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer learning using VGG16","metadata":{}},{"cell_type":"code","source":"# loading train images\nx_train_vgg, x_test_vgg, y_train_vgg, y_test_vgg = read_and_normalize_train_data(img_rows, img_cols, 3)\n\n# loading validation images\n# test_files, test_targets = read_and_normalize_sampled_test_data(nb_test_samples, img_rows, img_cols, 3)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:27:46.549631Z","iopub.execute_input":"2022-07-09T14:27:46.550389Z","iopub.status.idle":"2022-07-09T14:32:56.676755Z","shell.execute_reply.started":"2022-07-09T14:27:46.55035Z","shell.execute_reply":"2022-07-09T14:32:56.675745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = Traindatagen.flow(x_train_vgg, y_train_vgg, batch_size = 256)\nval_generator = Valdatagen.flow(x_test_vgg, y_test_vgg, batch_size = 64)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:11.495174Z","iopub.execute_input":"2022-07-09T14:33:11.495801Z","iopub.status.idle":"2022-07-09T14:33:12.681053Z","shell.execute_reply.started":"2022-07-09T14:33:11.495755Z","shell.execute_reply":"2022-07-09T14:33:12.68011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 40\ntrain_bs = 256\nvaldi_bs = 64","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:19.258097Z","iopub.execute_input":"2022-07-09T14:33:19.258485Z","iopub.status.idle":"2022-07-09T14:33:19.263267Z","shell.execute_reply.started":"2022-07-09T14:33:19.258453Z","shell.execute_reply":"2022-07-09T14:33:19.26225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\n\ndef classificationModel():\n    inp = keras.layers.Input(shape=(128, 128, 3))\n    vgg = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_tensor = inp,\n                  input_shape=(128, 128, 3))\n    vgg.trainable = False\n    \n    x = vgg.get_layer('block5_pool').output\n    x = tf.keras.layers.Flatten()(x)\n    # x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(256, activation='relu')(x)\n    output = keras.layers.Dense(10, activation='softmax')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs=output)\n\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:22.946653Z","iopub.execute_input":"2022-07-09T14:33:22.947763Z","iopub.status.idle":"2022-07-09T14:33:22.955976Z","shell.execute_reply.started":"2022-07-09T14:33:22.947671Z","shell.execute_reply":"2022-07-09T14:33:22.955043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nmodel = classificationModel()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:24.231715Z","iopub.execute_input":"2022-07-09T14:33:24.232061Z","iopub.status.idle":"2022-07-09T14:33:24.521063Z","shell.execute_reply.started":"2022-07-09T14:33:24.232032Z","shell.execute_reply":"2022-07-09T14:33:24.519927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n# model.compile(optimizer='rmsprop',\n#                 loss='categorical_crossentropy',\n#                 metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:26:34.765073Z","iopub.execute_input":"2022-07-09T14:26:34.765861Z","iopub.status.idle":"2022-07-09T14:26:34.771051Z","shell.execute_reply.started":"2022-07-09T14:26:34.765822Z","shell.execute_reply":"2022-07-09T14:26:34.769259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=opt,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:34.788534Z","iopub.execute_input":"2022-07-09T14:33:34.788917Z","iopub.status.idle":"2022-07-09T14:33:34.800233Z","shell.execute_reply.started":"2022-07-09T14:33:34.788886Z","shell.execute_reply":"2022-07-09T14:33:34.799179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0001,\n    patience=3,\n    verbose=1,\n    mode='min',\n    baseline=None,\n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:35.275986Z","iopub.execute_input":"2022-07-09T14:33:35.276335Z","iopub.status.idle":"2022-07-09T14:33:35.281338Z","shell.execute_reply.started":"2022-07-09T14:33:35.276307Z","shell.execute_reply":"2022-07-09T14:33:35.280418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN Visualization","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:36.719098Z","iopub.execute_input":"2022-07-09T14:33:36.719845Z","iopub.status.idle":"2022-07-09T14:33:47.280543Z","shell.execute_reply.started":"2022-07-09T14:33:36.71981Z","shell.execute_reply":"2022-07-09T14:33:47.279356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom wandb.keras import WandbCallback\n\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:33:47.282574Z","iopub.execute_input":"2022-07-09T14:33:47.282883Z","iopub.status.idle":"2022-07-09T14:33:56.625719Z","shell.execute_reply.started":"2022-07-09T14:33:47.282854Z","shell.execute_reply":"2022-07-09T14:33:56.624743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GradCAM:\n    \"\"\"\n    Reference:\n        https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\n    \"\"\"\n\n    def __init__(self, model, layerName):\n        self.model = model\n        self.layerName = layerName\n\n        self.gradModel = tf.keras.models.Model(inputs=[self.model.inputs],\n                                               outputs=[self.model.get_layer(self.layerName).output, self.model.output])\n\n    def compute_heatmap(self, image, classIdx, eps=1e-8):\n        with tf.GradientTape() as tape:\n            tape.watch(self.gradModel.get_layer(self.layerName).variables)\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = self.gradModel(inputs)\n\n            if len(predictions) == 1:\n                # Binary Classification\n                loss = predictions[0]\n            else:\n                loss = predictions[:, classIdx]\n\n        grads = tape.gradient(loss, convOutputs)\n\n        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n        castGrads = tf.cast(grads > 0, \"float32\")\n        guidedGrads = castConvOutputs * castGrads * grads\n\n        convOutputs = convOutputs[0]\n        guidedGrads = guidedGrads[0]\n\n        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n\n        (w, h) = (image.shape[2], image.shape[1])\n        heatmap = cv2.resize(cam.numpy(), (w, h))\n\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n        heatmap = (heatmap * 255).astype(\"uint8\")\n\n        return heatmap\n\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_HOT):\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n\n        return (heatmap, output)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:34:39.452292Z","iopub.execute_input":"2022-07-09T14:34:39.453398Z","iopub.status.idle":"2022-07-09T14:34:39.476996Z","shell.execute_reply.started":"2022-07-09T14:34:39.453354Z","shell.execute_reply":"2022-07-09T14:34:39.475762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GRADCamLogger(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data, layer_name):\n      super(GRADCamLogger, self).__init__()\n      self.validation_data = validation_data\n      self.layer_name = layer_name\n\n    def on_epoch_end(self, logs, epoch):\n      images = []\n      grad_cam = []\n\n      ## Initialize GRADCam Class\n      cam = GradCAM(model, self.layer_name)\n\n      for image in self.validation_data:\n        image = np.expand_dims(image, 0)\n        pred = model.predict(image)\n        classIDx = np.argmax(pred[0])\n  \n        ## Compute Heatmap\n        heatmap = cam.compute_heatmap(image, classIDx)\n        \n        image = image.reshape(image.shape[1:])\n        image = image*255\n        image = image.astype(np.uint8)\n\n        ## Overlay heatmap on original image\n        heatmap = cv2.resize(heatmap, (image.shape[0],image.shape[1]))\n        (heatmap, output) = cam.overlay_heatmap(heatmap, image, alpha=0.5)\n\n        images.append(image)\n        grad_cam.append(output)\n\n      wandb.log({\"images\": [wandb.Image(image)\n                            for image in images]})\n      wandb.log({\"gradcam\": [wandb.Image(cam)\n                            for cam in grad_cam]})","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:34:58.577038Z","iopub.execute_input":"2022-07-09T14:34:58.577389Z","iopub.status.idle":"2022-07-09T14:34:58.587832Z","shell.execute_reply.started":"2022-07-09T14:34:58.577361Z","shell.execute_reply":"2022-07-09T14:34:58.586736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Prepare sample images to run your GradCam on. \nsample_images, sample_labels = val_generator[20]\nsample_images.shape, sample_labels.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:35:00.90447Z","iopub.execute_input":"2022-07-09T14:35:00.90523Z","iopub.status.idle":"2022-07-09T14:35:00.9252Z","shell.execute_reply.started":"2022-07-09T14:35:00.905193Z","shell.execute_reply":"2022-07-09T14:35:00.924071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Interactive Monitoring Window <br>\nYou can scroll up and down across charts, view system utilization, model architecture. <br>\n**First Set of Panels (Charts):** <br>\n> 1. Training accuracy, Validation accuracy vs Step\n> 2. Training loss, Validation loss vs Step\n> 3. Epochs vs Step\n\n> **Second Set of Panels (Activation maps):** <br>\n> 1. Gradient Cam activation maps per class vs Step\n> 2. Images vs Step\n> 3. Examples vs Step","metadata":{}},{"cell_type":"code","source":"wandb.init(project=\"test\", entity=\"team-7\")","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:35:08.169084Z","iopub.execute_input":"2022-07-09T14:35:08.170212Z","iopub.status.idle":"2022-07-09T14:35:11.635083Z","shell.execute_reply.started":"2022-07-09T14:35:08.170154Z","shell.execute_reply":"2022-07-09T14:35:11.63373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Accuracy <br>\n- The transfer Learning using VGG16 conv_base and the augmented data reached a validation **accuracy** of **98.42%**","metadata":{}},{"cell_type":"code","source":"# History_Rs = resnetRs.fit(train_generator,\n#          validation_data=val_generator,\n#          steps_per_epoch=len(x_train_rs) // train_bs, epochs=epochs,\n#           validation_steps =len(x_test_rs)//valdi_bs, verbose = 1)\n\n\nhistory = model.fit(train_generator,\n                          validation_data=val_generator,\n                          steps_per_epoch=len(x_train_vgg) // train_bs, \n                          epochs=epochs,\n                          validation_steps =len(x_test_vgg)//valdi_bs, \n                          verbose = 1,\n                          callbacks=[WandbCallback(data_type=\"image\", validation_data=(sample_images, sample_labels)),\n                                     GRADCamLogger(sample_images, layer_name='block5_conv3'),\n                                     early_stopping])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T14:35:36.503612Z","iopub.execute_input":"2022-07-09T14:35:36.50438Z","iopub.status.idle":"2022-07-09T15:22:48.563222Z","shell.execute_reply.started":"2022-07-09T14:35:36.50434Z","shell.execute_reply":"2022-07-09T15:22:48.561254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('./vgg_tl_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = tf.keras.models.load_model('./my_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-07-08T15:06:00.280427Z","iopub.execute_input":"2022-07-08T15:06:00.280931Z","iopub.status.idle":"2022-07-08T15:06:00.645273Z","shell.execute_reply.started":"2022-07-08T15:06:00.280886Z","shell.execute_reply":"2022-07-08T15:06:00.644115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T15:26:54.973706Z","iopub.execute_input":"2022-07-09T15:26:54.974066Z","iopub.status.idle":"2022-07-09T15:26:55.641773Z","shell.execute_reply.started":"2022-07-09T15:26:54.974036Z","shell.execute_reply":"2022-07-09T15:26:55.640898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}